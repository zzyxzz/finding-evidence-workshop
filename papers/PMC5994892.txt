Sharing and reusing cell image data 
The rapid growth in content and complexity of cell image data creates an opportunity for synergy between experimental and computational scientists. 
Sharing microscopy data enables computational scientists to develop algorithms and tools for data analysis, integration, and mining. 
These tools can be applied by experimentalists to promote hypothesis-generation and discovery. 
We are now at the dawn of this revolution: infrastructure is being developed for data standardization, deposition, sharing, and analysis; some journals and funding agencies mandate data deposition; data journals publish high-content microscopy data sets; quantification becomes standard in scientific publications; new analytic tools are being developed and dispatched to the community; and huge data sets are being generated by individual labs and philanthropic initiatives. 
In this Perspective, I reflect on sharing and reusing cell image data and the opportunities that will come along with it. 
BACKGROUND 
Molecular cell biology and microscopy have undergone a revolution that led to an explosion in complex, dynamic, high-dimensional imaging data (Reynaud et al., 2015; Ouyang and Zimmer, 2017). 
The lack of computational methods to extract information from such rich and high-content data is now becoming a critical bottleneck, and thus the field of cell imaging is in great need of computational scientists. 
However, there is a huge gap between biologists who produce, analyze, and hold the data, and computational scientists whose technical and analytical skills can enable extraction of more information from it (Figure 1). 
This gap is caused by differences in culture, communication, academic motivation, and reward. 
The gap between cell biology and computer science has roots in different cultural aspects and lack of cross-discipline communication. 
Availability of large-scale data sets will make a significant step toward bridging this gap. 
Scientists with computational backgrounds (CS, computer science) will be motivated to exercise their skills in data integration, mining, and tool development to benefit cell biology (BIO) through availability of new computational tools to analyze and interpret cell image data. 
Credit: Dorit Kochavi. 
One key step toward filling this gap is making cell image data publicly available. 
Data availability will attract computational scientists by exposing them to fresh and challenging problems at the interface of computer vision, data science, and cell biology. 
Just as in the emergence of bioinformatics, data availability will likely first engage computational scientists in development of tools and methods for analysis and data mining, before diving into deeper biological waters: integrating multiple data sets and examining “old” data from new perspectives to make new discoveries. 
Data depositors will profit from increased academic credit in publications, citations, and new collaborations. 
Cell biologists will enjoy the availability of new computational methods and complementary data sets to reproduce and validate their findings. 
This synergy will benefit all parties and move cell biology forward. 
Deposition of data in public repositories upon publication has become the standard in many fields such as gene expression, three-dimensional protein structure and proteomics. 
Sharing enables reanalysis of the data by other scientists for replication, computational tool development and turning data to discovery. 
This is not the case for cell image data, where the complexity, multidimensionality, variability in experimental settings, lack of standardization and huge content complicate sharing and reuse. 
Here, I discuss the barriers toward open cell image data, the steps that are required to enable effective sharing and reuse, and the expected benefits to follow. 
The impact of image data sharing 
Public availability of cell image data is essential for improving reproducibility, assessment, and validation of new computational methods; integration and mining multiple data sets; and quantitatively examining previously published data from new angles to facilitate discovery (Pasquetto et al., 2017). 
First and most obvious, deposition of image data in public repositories can help with what is referred to as the “reproducibility crisis” (Baker, 2016). 
Open data can reduce data cherry picking, enable independent validation of previous research outputs, simplify replication studies and allow generalization of conclusions to additional cell or experimental systems. 
Most computational scientists in the field of cell imaging are focused on developing analytic tools for common universal computational problems such as preprocessing steps, registration, detection, segmentation, tracking, feature extraction, and classification (Meijering et al., 2016). 
A critical aspect when presenting a new method is comparing its performance to alternative approaches. 
Accordingly, most current examples of reusing cell image data are aimed at the validation and assessment of new computational tools. 
For example, the Mitocheck project created a resource of genomewide phenotypic profiling (Neumann, Walter, et al., 2010). 
Its image and image-derived data were reused to develop multiple methods such as inferring gene networks (Failmezger, Praveen et al., 2013b), predicting gene function from RNAi-induced phenotypic similarities (Serrano-Solano et al., 2017), unsupervised phenotyping (Failmezger et al., 2013a), quantification of single cell motility in high-throughput time-lapse screening data (Schoenauer Sebag et al., 2015), and cell tracking (Lou and Hamprecht, 2011) (that also reused data from Li et al., 2010). 
WND-CHARM, an image classification framework (Orlov et al., 2008), used published cell images for benchmarking (Boland et al., 1998; Boland and Murphy, 2001), and CP-CHARM, a CellProfiler-based image classification method (Uhlmann et al., 2016), was validated also with additional data sets from the Broad Bioimage Benchmark Collection (Ljosa et al., 2012). 
AveMap, a method to quantify monolayer migration (Deforet, Parrini, et al., 2012) was verified on data from Simpson et al. (2008). Osokin et al. (2017) applied deep learning to infer the localization of one protein based on the spatial pattern of another protein; to train their model they used an existing data set (Dodgson, Chessel, Vaggi, et al., 2017). 
Community competitions and benchmarking efforts, using curated standardized data, ground-truth annotations, and performance metrics, have proven effective at objectively comparing methods for particle tracking (Chenouard, Smal, De Chaumont, Maška, et al., 2014), single molecule localization (Sage et al., 2015), cell tracking (Ulman, Maška, et al., 2017), nucleus detection (www.kaggle.com/c/data-science-bowl-2018), and other methods (Meijering et al., 2016). 
The benefit of competitions is twofold: users can more effectively select an existing method that is likely to perform well for their specific data, and method developers can demonstrate superiority of their method—a critical aspect for publication in the fields of applied computational sciences. 
Secondary analysis of data can also produce new biological insight. 
In this mode, researchers extract new information from raw image data or image-derived features (e.g., cell/molecular trajectories, segmentation masks) by applying new analyses that were not considered in the original study. 
For example, data from the First World Cell Race (Maiuri et al., 2012), a large-scale comparison of cell motility across 54 different adherent cell types, was reused by the same group to confirm an association between cell speed and persistent migration and then extended by a set of new experiments and theory to reveal a universal coupling that is mediated by actin flows (Maiuri, Rupprecht, Wieser, et al., 2015). Lavi et al. (2016) introduced theory for competition between cell motility machinery and microbial antigen capture for myosin II. 
Their prediction that cells switch from persistent migration to unidirectional self-oscillation was validated by reanalyzing cell trajectories from Chabaud, Heuzé, et al. (2015). Meyers, Craig, and Odde (2006) tested their theoretical predictions linking cell size and shape to signaling by new analyses of previous data of Cdc42 activation in fibroblasts (Nalbant, Hodgson, et al., 2004). 
Ji et al. reanalyzed raw dual-channel time-lapse imaging (Hu, Ji, et al., 2007) to dissect the relationship between predicted adhesion forces and F-actin-vinculin interactions (Ji et al., 2008). 
Abdullah et al. segmented individual cells in an epithelial tissue reusing time lapse images of developing Drosophila pupae from Besson, Bernard, et al. (2015). 
They demonstrated that the probability of cell division increases exponentially with the number of cell edges and developed theory to propose that this is responsible for the observed cell-edge distribution (Abdullah et al., 2017). 
Thurley et al. developed “response-time modeling” as a framework to unify and interpret knowledge on intra- and intercellular signaling pathways (Thurley et al., 2018) and applied it to published experimental and simulated cytokine secretion data (Dorner, Dorner, Zhou, et al., 2009; Busse et al., 2010; Han, Bagheri, et al., 2012; Thurley Gerecht, Friedmann, and Höfer, 2015). 
Yang and Svitkina carefully reassessed published electron tomography data from Urban et al. (2010) and reported the existence of numerous branched actin filaments in lamellipodia that have been overlooked in the original study reporting their absence (Yang and Svitkina, 2011). 
In my own research, I reanalyzed data from Serra-Picamal, Conte, et al. (2012) to test plithotaxis—the tendency for each individual cell within a collectively migrating monolayer to migrate along the local orientation of the maximal principal stress (Tambe, Hardin, et al., 2011). 
I found that plithoatxis is a property attributed to a small subgroup of cells that migrate more effectively (Zaritsky et al., 2015b). 
By designing new algorithms, I discovered that coordinated stress precedes coordinated motion and by reanalyzing published data from another group (Das et al., 2015), proposed that several tight-junction proteins play a role in transmission of aligned stress to aligned motion (Zaritsky et al., 2015b). 
I also used the same data to validate a new method developed for quantification of colocalization and coalignment data (Zaritsky et al., 2017). 
Integrating data sets from multiple sources have the potential to discover patterns that are not possible to infer from individual studies (Lahat et al., 2015). Williams, Moore, Li, et al. (2017) recently illustrated the benefit of image-derived data integration. 
By combining information from three independent data sets (Fuchs, Pau, et al., 2010; Rohn et al., 2011; Graml, Studera, Lawson, Chessel, et al., 2014), the authors suggested a new gene-network controlling cell shape that could not be inferred from single studies. 
This first example is setting the stage for future ambitious “meta-analysis” studies, for example, integration of imaging and omics-based data sets for system-level genotype-phenotype mapping. 
Current advances in cell biology resemble the famous parable of the blind men and an elephant, in which each blind man inspects a different part of the elephant body, revealing a limited aspect of reality. 
Conceptualizing the elephant requires integration of all partial observations. 
Similarly, the ultimate impact in opening cell image data could come from the integration of partial observations to a complete understanding of the “biological scene.” This will be achieved by the integration of complementary data and by the use of complementary tools that examine data from different perspectives. 
Barriers and solutions toward open cell image data 
Cultural conventions and the lack of infrastructure are the main hurdles limiting image data sharing and reuse. 
In recent years we have witnessed independent seeds of infrastructure building, generation of high-content data sets and a growing appreciation of interdisciplinary and collaborative science. 
Together these seeds mark a beginning of a time to open cell image data and import concepts of “big data” science to cell biology. 
Convincing biologists to open their data is not trivial. 
Historically, cell biology lacks a culture of data sharing, and experimentalists are still accustomed to holding on to their image data. 
For example, in response to my request for published image data (in a “glamour” journal with a “data availability on request” statement), the corresponding author refused with the argument that “This might not be the best way to optimize scientific progress, but given the current rules of the game, it is what it is.” This is apparently a more general problem, not limited solely to cell image data, as implicated by a recent notorious editorial that raised the concern that “the system will be taken over” by what they called “research parasites” (Longo and Drazen, 2016). 
As a nice response, Casey Greene (Pennylvania State University) has initiated a “Parasite Award” (http://researchparasite.com/) for scientists practicing secondary data analysis and the “Symbiont Award” (http://researchsymbionts.org/) for experimentalists who shared their data. 
Although the vast majority of experimentalists agree with the general concept that data produced from public funds should become publicly available for general reuse, many feel genuinely frustrated about the idea of putting in the extra work to allow others to benefit from the expensive and labor-intensive data they generated. 
More specific concerns are that the lack of detailed understanding of the primary research could lead to erroneous conclusions and competition with others when additional analyses were planned (Longo and Drazen, 2016). 
Rewarding the sharing of primary research data could be key in changing this paradigm (Wallis et al., 2013). 
For example, it is established that papers with open data receive more citations over long time frames (Piwowar and Vision, 2013); funding agencies could credit researchers whose data are reused by others. 
To reach solid conclusions, data scientists must understand the full extent of the biological complexity in the data (Zaritsky, 2016), which almost always require direct communication with the data generators. 
Such interaction could even lead to closer partnerships and sought-after experimental validations of hypotheses that emerged from the secondary analyses. 
Thus, researchers who practice secondary analyses should strive to involve the primary data generators as collaborators and share credit. 
Image data sets with potential for reuse have scientific value on their own and so deserve direct academic credit. 
Accordingly, on deposition, data sets are assigned a unique identifier (doi) that can be later referenced independently of the scientific study. BMC Research Notes was the first journal to identify the potential impact, introducing “data format” as a new article type (O’Donnell et al., 2008). 
Since then, many journals introduced Resources or Tools/Methods/Software article types. 
GigaScience was the first to focus on data and other research products as its main publication entity, also providing means for data deposition (Sneddon et al., 2012) (http://gigadb.org/). 
This initiative was followed by Nature’s Scientific Data. 
Data sets that are selected for publication must follow the findable, accessible, interoperable, and reusable (FAIR) principles (Wilkinson et al., 2016) and be assessed based on their soundness and potential for future reuse. 
Examples of cell-image-based data sets include Zaritsky et al. (2015a), Bray et al. (2017), Pascual-Vargas et al. (2017), and Lukeš et al. (2018). 
It is not only a change of culture that is needed. 
The size, complexity, variability, and lack of standardized formats and metadata make the deposition process labor intensive and tedious, thus discouraging experimentalists. 
Data curation and deposition should become as simple and straightforward as possible and ideally provide academic reward to encourage experimentalists to share their data. 
Unlike omics data, until very recently there was no public repository dedicated for large-scale imaging data (Lemberger, 2015). 
The Image Data Resource (https://idr.openmicroscopy.org/about/) (Williams, Moore, Li, et al., 2017) is an open online platform for publishing, visualizing, and mining high-content cell image data sets. 
It contains curated large-scale data sets with raw and processed image data, together with phenotypic annotations, standardized vocabulary, and software infrastructure to allow straightforward querying and visualization. 
Earlier image repositories did not provide a complete suite to enable these functionalities. 
These include Yale Image Finder (Xu et al., 2008), retrieving images based on their textual description; PhenoImageShare (Adebayo et al., 2016), the first to provide infrastructure linking annotated ontologies-based tags to phenotypic-based user queries; the JCB Data Viewer (http://jcb-dataviewer.rupress.org/) (Hill, 2008); The Cell Image Library (http://www.cellimagelibrary.org/) (Orloff et al., 2013); and the Systems Science of Biological Dynamics database (http://ssbd.qbic.riken.jp) (Tohsato, Ho, et al., 2016). 
The Broad BioImage Benchmark Collection (https://data.broadinstitute.org/bbbc/) (Ljosa et al., 2012) is a resource for methods benchmarking. 
It includes raw data sets, ground-truth annotations for benchmarking and criteria for evaluating each data set. 
The complexity and costs of storing and managing large-scale image data cannot be overemphasized. 
In the near future, storage requirements, even at a single institution level, will easily exceed the petabyte (1 PB = 1000 TB) scale (Ouyang and Zimmer, 2017). 
Such content poses great challenges in terms of storage, retrieval, and mining. 
In a recent white paper, Ellenberg, Swedlow, et al. (2018) proposed a two-layered model where a data archive is used for data and metadata storage and access and added-value databases, a subset of the data archive, is identified as having greater potential for reuse by the community and provided with additional curation, annotation, and standardization. 
Obvious data sets that fit these criteria include atlases and high-content genetic phenotypic screens. 
Recent advances in molecular biology, microscopy, and automation enable the generation of such data sets even at individual labs (e.g., Cai, Hossain, et al., 2017). 
However, large-scale imaging consortia, community efforts, and philanthropic projects have the potential to produce larger-scale and more controlled image-data resources. 
One example is the Allen Institute of Cell Science, having the ultimate goal of building an integrated model of cell structure, organization, and function. 
Toward this goal, they genome-edited human stem cells (Roberts, Haupt, et al., 2017) and made highly standardized microscopy raw data and image-derived features publicly available through their Cell Explorer (www.allencell.org/image-data-downloads.html) (Horwitz, 2016). 
Other examples include Euro-Bioimaging (www.eurobioimaging.eu/), the Chan Zuckerberg initiative (https://chanzuckerberg.com/) (Bargmann, 2018), MultiMOT (https://multimot.org/) (Masuzzo and Martens, 2015), and the Human Cell Atlas (www.humancellatlas.org/) (Regev et al., 2017). 
The reuse potential of more specific imaging studies is less obvious and archiving would be determined per-case based on the community priorities (Ellenberg, Swedlow, et al., 2018). 
Infrastructure such as SourceData (Liechti, George, Götz, El-Gebali, et al., 2017), which links published figures to their underlying source data, can improve reproducibility in cases where the raw data are not archived. 
Storage costs and personnel (software engineers, data curators, and high performance computing specialists) are the major financial expenses essential to build and maintain large data repositories. 
Involvement of government support, funding agencies, industrial partners, and philanthropy is crucial, especially for the recognition and support of software development and high-performance computing needed to embed informatics as an integral part of advancing cell biology and for long-term maintenance of large-scale repositories and tools (Cardona and Tomancak, 2012; Prins, De Ligt, et al., 2015). 
A key aspect toward successful data dissemination and mining is organizing the metadata for flexible retrieval and interrogation, while keeping data deposition simple and fast. 
This fine balance poses challenges in data standardization, storage, and retrieval even more prominently in the complex landscape of cell imaging. 
Standardized data formats, community reporting guidelines, Application programming interfaces (APIs, which are software infrastructure to simplify use to technologies in developing applications), and visualization tools must be defined and developed toward this goal. 
Standard domain terminology, formally termed controlled vocabulary, enables harmonized data representation, which is necessary for querying across data sets. 
Many of the terms in a controlled vocabulary can be borrowed from existing ontologies, which are formalized hierarchical descriptions of a specific domain. 
Several ontologies describing experimental assays, cell types, and their phenotypes and behavior were recently defined (Visser et al., 2011; Hoehndorf et al., 2012; Sarntivijai et al., 2014; Sluka et al., 2014; Jupp et al., 2016), and new ontologies can be constructed on need. 
The minimal reporting requirements define the smallest set of metadata required to enable future data querying and integration. 
These include information on the model system, experiments, and microscopy used to generate the data, all in terms of the controlled vocabulary. 
Deciding on the minimal set tunes reuse possibilities with ease of data submission and should be carefully determined. 
Cell image data come in a wide range of proprietary and open file formats (Linkert et al., 2010). 
Controlled vocabularies can also aid in standardizing these data formats and implementing APIs to handle this data harmonization (Orchard et al., 2005). 
A successful example is the Open Microscopy Environment (Goldberg et al., 2005), providing the software infrastructure to store, share, and query image data from different sources. 
Distribution of image-derived features along the raw image data can be extremely beneficial in terms of reuse. 
The Cell Migration Standardization Organization (CMSO), a community effort toward the development of community standards for the field of cell migration that I am part of, has currently taken the challenge of defining and implementing data formats to harmonize routine analyses outputs, starting with a data format for trajectories of cells or molecular events (https://github.com/CellMigStandOrg/biotracks). 
Similar steps have been made by Rigano and Strambio-De-Castillia, who just released a cross-platform data management infrastructure for particle tracking data (http://omega.umassmed.edu/) (Rigano et al., 2018), using their minimal reporting requirements (Rigano and De Castillia, 2017). 
Efforts have been also made in the arena of mathematical modeling (Macklin and Friedman, 2018), for example, Multi Cellular Data Standard, a new data format for multicellular data (Friedman et al., 2016). 
Altogether, the emerging infrastructures of data-rich repositories, controlled vocabularies, minimal reporting requirements, and APIs will enable data-focused exploratory or hypothesis-driven queries that will lead to new discoveries. 
Recruiting computational scientists to cell biology through open data 
The question of how far secondary analysis can take us has already been answered in other fields. 
For example, in the omics fields, computational scientists who never held a pipette can be the ones driving the biological interrogation. 
They, of course come along with others who develop computational tools and who provide bioinformatics services in core facilities, as a range of individuals contributing in different ways to the field. 
For cell imaging, even when the data become widely available, cultural barriers still stand in the way of attracting computational scientists, including appreciation of secondary-analysis research by the cell biology community, adequate academic reward and career opportunities, and cross-disciplinary training. 
Development of tools facilitate access to information previously unattainable and so can be more scientifically impactful than making a scientific discovery. 
In the past few years we have seen Nobel laureates who developed cryoelectron and superresolved fluorescence microscopy. 
We have also witnessed how software tools, such as BLAST (sequence analysis) (Altschul et al., 1990), changed the way that science is performed and greatly helped in the emergence of bioinformatics as an independent field. 
Public data and benchmarks are necessary to make this leap in what is called bioimage informatics (Peng, 2008), but this is not sufficient. 
Adaptation of existing algorithms to cell image data, dealing with the inherent variability and noise in experimental biology and paying attention to software engineering and usability usually lacks the mathematical rigor and novelty sought in top-tier computer science, while also lacking the biological novelty sought in cell biology (Cardona and Tomancak, 2012). 
Another discrepancy comes from the underlying motivation: for applied computer scientists elegant math and outperforming the state of the art (even by a margin) are the goals, whereas studies in cell biology are motivated by better understanding of a specific biological process, using whatever available techniques (Meijering et al., 2016). 
We have seen multiple success stories in the past 15 years—bioimage analysis tools that are used by thousands of scientists around the globe (e.g., Carpenter et al., 2006; Sommer et al., 2011; de Chaumont et al., 2012; Schneider et al., 2012). 
A positive action was made by journals offering new article types focusing on tool and software development, for example, PLoS Biology’s “Meta-Research” section on data-driven and meta-analytic research (Kousta et al., 2016). 
Another positive sign is the growing community of bioimage analysts (in Europe, NEUBIAS, http://neubias.org/), as a bridge between tool developers and biology end users. 
Roles of computational scientists in cell biology can go beyond number crunching, statistical analyses, and tool development. 
The mass and complexity of microscopy data create an opportunity for computational scientists to decipher fundamental biological processes by analyzing image data. 
Of course, discovery of complex dynamic patterns requires deep domain knowledge of the biological process, the experimental possibilities, the type of information that can be extracted, and the computational tools to extract it. 
This notion was shouted out in several recent opinion pieces. 
Markowetz (2017) argued that “computational biologists are just biologists using a different tool,” and I introduced the “dry cell biologist” and its role in promoting interdisciplinary team science (Zaritsky, 2016), which was also proposed as an evident mode of performing modern cell biology by Horwitz (2016). 
To facilitate this type of research, results should not be assessed based on the amount of new data generated but instead on the findings and biological insight extracted from the data regardless of the purpose for which they were originally generated. 
This problem was evidently stated by an anonymous reviewer of one of my manuscripts, “…a major criticism of the current manuscript, which identifies real and important relations, is using previously published data instead of independently performing the experiment.” 
Big data are not the end of hypothesis-driven science (Mazzocchi, 2015). 
It is true that mining through massive cell image public data sets, extracting complex patterns and turning it to new biological insight, is going to become a more prevalent mode of science in the near future. 
However, the concern that discovery-driven data science may find spurious correlations with no biological interpretation is real and must be addressed by a careful examination of different experimental methods and systems. 
For a dry cell biologist, this can be achieved by engaging an experimentalist collaborator to jointly decipher a pattern discovered via mining existing data or, alternatively, by finding independent existing data sets and developing new analyses to test the hypotheses from different angles. 
After all is said and done, the most influential “carrots and sticks” toward open data and engaging scientists with solid computational background in cell biology are in the hands of journals and funding agencies. 
Similarly to the omics fields, they should become more involved: funders should require the sharing of cell image data, and journals must enforce public dissemination to ensure reproducibility. 
Funding agencies should promote collaborative projects, fund software solutions for cell imaging big data, and explicitly support maintenance of such projects and experts in high-performance computing and software engineering (Cardona and Tomancak, 2012; Prins, De Ligt, et al., 2015). 
New multidisciplinary education and training programs must be established to bridge the technical and cultural gap between the disciplines (Meijering et al., 2016). 
Now is the time! 
New techniques in genome engineering and microscopy facilitate the generation of high-content cell image data. 
At the same time, cell biology is advancing to more physiologically relevant and complex systems. 
Together, the content and complexity of this new generation of cell image data are making visual assessment impossible. 
Data scientists are needed to develop tools to quantify these data and decipher the complex patterns that are encapsulated in them. 
One key step toward engaging scientists from a computational background to cell biology is to open data. 
These issues were discussed in a special interest subgroup that I organized last year at the American Society for Cell Biology (ASCB)|EMBO annual meeting (https://assafzar.wixsite.com/ascb2017-subgroup), during which three key components were highlighted (Figure 2): 1) changing cultural barriers regarding sharing primary research data, rewarding depositors, recognition of results derived by secondary analysis and scientists who specialize in it, and improving communication between the fields; 2) building infrastructure to enable easy data deposition and mining: repositories, standardized data formats, APIs and visualization tools; and 3) developing new computational methods to deal with the inherent complexity and variability of these data. 
Although many encouraging signs suggest that the field is moving in this direction, there are still plenty of challenges ahead. 
Cell image data science is expected to face similar computational challenges to those of other “big data” fields, such as genomics, experience in data acquisition, storage, distribution, and analysis (Stephens et al., 2015). 
Exciting times lie ahead of us—come and join in! 
The components needed to bring data science to cell biology and the role of sharing image data for secondary analysis. 
DOI:10.1091/mbc.E17-10-0606 
